{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import sys\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import cv2\n",
    "import time\n",
    "import functools\n",
    "import datetime\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.initializers import *\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT = \"CartPole-v0\"\n",
    "\n",
    "# Overall\n",
    "SEED = 789325\n",
    "SAVE_EVERY_EPISODES = 100\n",
    "UPDATE_TARGET_FREQUENCY = 10\n",
    "\n",
    "# CartPole-v0\n",
    "IS_CNN = False\n",
    "LEARNING_START_AFTER_STEPS = 500\n",
    "EPISODES = 110\n",
    "SCORE_TO_SOLVE = 195.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Environment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:  2\n",
      "Size of state: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENVIRONMENT)\n",
    "env.seed(SEED)\n",
    "\n",
    "# size of each action\n",
    "action_size = env.action_space.n\n",
    "print('Actions: ', action_size)\n",
    "if hasattr(env.env, 'get_action_meanings'):\n",
    "    print(env.env.get_action_meanings())\n",
    "\n",
    "# examine the state space \n",
    "states = env.observation_space.shape\n",
    "state_size = states[0]\n",
    "print('Size of state:', state_size)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    data_pointer = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update (tree_index, priority)\n",
    "        self.data_pointer += 1\n",
    "        \n",
    "        if self.data_pointer >= self.capacity:\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    def update(self, tree_index, priority):\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "\n",
    "        while tree_index != 0:\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "    \n",
    "    def get_leaf(self, v):\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True:\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            \n",
    "            else:                \n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                    \n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "            \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer(object):\n",
    "    PER_e = 0.01\n",
    "    PER_a = 0.6\n",
    "    PER_b = 0.4\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        \n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "        \n",
    "        self.tree.add(max_priority, experience)\n",
    "\n",
    "    def sample(self, n):        \n",
    "        memory_b = []        \n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)                \n",
    "        priority_segment = self.tree.total_priority / n    \n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])        \n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "                        \n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "                        \n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "                                   \n",
    "            b_idx[i]= index\n",
    "            \n",
    "            experience = [data]\n",
    "            \n",
    "            memory_b.append(experience)\n",
    "        \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size=2000):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        \n",
    "    def add(self, experience):        \n",
    "        if len(self.buffer) <= self.max_size:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[0] = experience \n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return [], rn.sample(self.buffer, batch_size), []\n",
    "    \n",
    "    def batch_update(self, indices, td_errors):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, \n",
    "                 state_size,\n",
    "                 action_size,\n",
    "                 model_builder_func,\n",
    "                 preprocess_func,\n",
    "                 use_PER=True,\n",
    "                 buffer_size=10000,\n",
    "                 batch_size=32,\n",
    "                 gamma=0.99,\n",
    "                 epsilon_start=1.0,\n",
    "                 epsilon_min=0.1,\n",
    "                 epsilon_steps_to_min=1000,\n",
    "                 tau=0.1,                \n",
    "                 pre_trained=None):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.use_PER = use_PER\n",
    "        \n",
    "        if self.use_PER:\n",
    "            self.replay_buffer = PrioritizedReplayBuffer(capacity=buffer_size)\n",
    "        else:\n",
    "            self.replay_buffer = Memory(max_size=buffer_size)        \n",
    "        \n",
    "        self.batch_size = batch_size        \n",
    "        self.gamma = gamma        \n",
    "        self.epsilon = epsilon_start        \n",
    "        self.epsilon_min = epsilon_min          \n",
    "        self.epsilon_step = (self.epsilon - self.epsilon_min) / epsilon_steps_to_min\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.model = model_builder_func(state_size, action_size, pre_trained)\n",
    "        self.target_model = model_builder_func(state_size, action_size, pre_trained)\n",
    "\n",
    "        self.preprocess_func = preprocess_func\n",
    "        \n",
    "        self.hard_update_target_network()\n",
    "    \n",
    "    def hard_update_target_network(self):    \n",
    "        pars = self.model.get_weights()\n",
    "        self.target_model.set_weights(pars)\n",
    "    \n",
    "    def soft_update_target_network(self):\n",
    "        pars_behavior = self.model.get_weights()\n",
    "        pars_target = self.target_model.get_weights()\n",
    "        \n",
    "        ctr = 0\n",
    "        for par_behavior,par_target in zip(pars_behavior,pars_target):\n",
    "            par_target = par_target*(1-self.tau) + par_behavior*self.tau\n",
    "            pars_target[ctr] = par_target\n",
    "            ctr += 1\n",
    "\n",
    "        self.target_model.set_weights(pars_target)\n",
    "       \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.add((state, action, reward, next_state, done)) \n",
    "          \n",
    "    def preprocess(self, state):\n",
    "        return self.preprocess_func(state, self.state_size)\n",
    "            \n",
    "    def act(self, state):\n",
    "        # Update exploration rate\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_step        \n",
    "        \n",
    "        # Choose Action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action = np.random.choice(self.action_size)\n",
    "        else:            \n",
    "            Qs = self.model.predict(state)[0]\n",
    "            action = np.argmax(Qs)\n",
    "                  \n",
    "        return action          \n",
    "    \n",
    "    def train(self):\n",
    "        indices, mini_batch, importance  = self.replay_buffer.sample(self.batch_size)\n",
    "                        \n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        \n",
    "        Q_wants = []\n",
    "        td_errors = np.zeros(self.batch_size)\n",
    "        \n",
    "        for i in range(len(mini_batch)):            \n",
    "            if not self.use_PER:\n",
    "                state, action, reward, next_state, done = mini_batch[i]\n",
    "            else:\n",
    "                state = mini_batch[i][0][0]\n",
    "                action = mini_batch[i][0][1]\n",
    "                reward = mini_batch[i][0][2]\n",
    "                next_state = mini_batch[i][0][3]\n",
    "                done = mini_batch[i][0][4]          \n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "                    \n",
    "        states_tensor = np.reshape(states,(self.batch_size,len(states[0])))\n",
    "        Q_wants_pred = self.model.predict(states_tensor)\n",
    "        \n",
    "        next_states_tensor = np.reshape(next_states,(self.batch_size,len(next_states[0])))\n",
    "        Q_next_state_vecs = self.model.predict(next_states_tensor)\n",
    "        Q_target_next_state_vecs = self.target_model.predict(next_states_tensor)\n",
    "            \n",
    "        for i in range(len(mini_batch)):\n",
    "            action = actions[i]\n",
    "            reward = rewards[i]\n",
    "            done = dones[i]\n",
    "            \n",
    "            Q_want = Q_wants_pred[i]\n",
    "            Q_want_old = Q_want[action]\n",
    "            \n",
    "            if done:\n",
    "                Q_want[action] = reward\n",
    "            else:\n",
    "                Q_next_state_vec = Q_next_state_vecs[i]\n",
    "                action_max = np.argmax(Q_next_state_vec)\n",
    "               \n",
    "                Q_target_next_state_vec = Q_target_next_state_vecs[i]\n",
    "                Q_target_next_state_max = Q_target_next_state_vec[action_max]\n",
    "               \n",
    "                Q_want[action] = reward + self.gamma*Q_target_next_state_max\n",
    "                Q_want_tensor = np.reshape(Q_want,(1,len(Q_want)))\n",
    "            \n",
    "            Q_wants.append(Q_want)            \n",
    "            td_errors[i] = abs(Q_want_old - Q_want[action])\n",
    "          \n",
    "        states = np.array(states)\n",
    "        Q_wants = np.array(Q_wants)\n",
    "        self.model.fit(states, Q_wants, verbose=False, epochs=1)\n",
    "\n",
    "        # update replay buffer\n",
    "        self.replay_buffer.batch_update(indices, np.array(td_errors))                \n",
    "    \n",
    "    def save(self, file='model.h5'):\n",
    "        print('Save model...')\n",
    "        self.model.save(file)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atari(object):\n",
    "    def __init__(self, envName, seed=None, clip_rewards=False, frame_skip=None, null_op_max=30, cnn=True):\n",
    "        self.env = gym.make(envName)        \n",
    "        if seed is not None:\n",
    "            self.env.seed(seed)\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.reset()        \n",
    "        return state\n",
    "                          \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Cartpole_v0_agent(pre_trained=None):\n",
    "    def build_model(state_size, action_size, pre_trained):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=state_size, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(action_size, activation='linear'))\n",
    "        if pre_trained:            \n",
    "            model = load_model(pre_trained)\n",
    "\n",
    "        model.compile(optimizer=Adam(lr=0.001), loss='mse')\n",
    "        return model    \n",
    "    \n",
    "    def build_model_duelling(state_size, action_size, pre_trained):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=state_size, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(action_size + 1, activation='linear'))        \n",
    "        model.add(Lambda(lambda i: K.expand_dims(i[:,0],-1) + i[:,1:] - K.mean(i[:,1:], keepdims=True), output_shape=(action_size,)))\n",
    "        \n",
    "        if pre_trained:            \n",
    "            model = load_model(pre_trained)\n",
    "\n",
    "        model.compile(optimizer=Adam(lr=0.001), loss='mse')        \n",
    "        return model\n",
    "    \n",
    "    def preprocess(state, state_size):\n",
    "        return np.reshape(state, [1, state_size])\n",
    "    \n",
    "    agent = DDQNAgent(state_size,\n",
    "                     action_size,\n",
    "                     build_model_duelling,\n",
    "                     preprocess,\n",
    "                     use_PER=True,\n",
    "                     buffer_size=2000,\n",
    "                     epsilon_start=0.5,\n",
    "                     epsilon_steps_to_min=3500,\n",
    "                     pre_trained=pre_trained)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs/\" + time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "#logdir = \"logs/\" + \"Duelling DDQN + PER loss='huber'\"\n",
    "#logdir = \"logs/\" + \"Duelling DDQN + PER loss='huber'\"\n",
    "writer = tf.summary.create_file_writer(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started: 2019-12-29 10:48:31.836427\n",
      "Episode 1: Step 15 reward 15.0: \n",
      "Save model...\n",
      "Episode 5: Step 80 reward 30.0: \n",
      "Save model...\n",
      "Episode 6: Step 123 reward 43.0: \n",
      "Save model...\n",
      "Episode 7: Step 180 reward 57.0: \n",
      "Save model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tobias\\appdata\\local\\conda\\conda\\envs\\openai\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "c:\\users\\tobias\\appdata\\local\\conda\\conda\\envs\\openai\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 46: Step 860 reward 58.0: \n",
      "Save model...\n",
      "Episode 47: Step 947 reward 87.0: \n",
      "Save model...\n",
      "Episode 61: Step 1757 reward 92.0: \n",
      "Save model...\n",
      "Episode 65: Step 2039 reward 105.0: \n",
      "Save model...\n",
      "Episode 67: Step 2269 reward 133.0: \n",
      "Save model...\n",
      "Episode 68: Step 2469 reward 200.0: \n",
      "Save model...\n",
      "Save after 100 episodes.\n",
      "Save model...\n",
      "Step count: 10000\n",
      "Save model...\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    env = Atari(ENVIRONMENT, seed=SEED, clip_rewards=False, frame_skip=None,  cnn=IS_CNN)\n",
    "    agent = build_Cartpole_v0_agent()\n",
    "    \n",
    "    max_reward = -9999999    \n",
    "    game_rewards_deque = deque(maxlen=100)    \n",
    "    frame_count = 0\n",
    "    \n",
    "    print(\"Training started: \" + str(datetime.datetime.now()))\n",
    "    \n",
    "    frame_count = 0\n",
    "    \n",
    "    for i_episode in range(1, EPISODES+1):\n",
    "        state = env.reset()\n",
    "            \n",
    "        game_reward = 0\n",
    "        ep_loss = []\n",
    "        \n",
    "        starttime = datetime.datetime.now()\n",
    "        \n",
    "        steps = 0\n",
    "        while True:\n",
    "            frame_count += 1\n",
    "            steps += 1\n",
    "            \n",
    "            state = agent.preprocess(state)                \n",
    "            action = agent.act(state)            \n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)              \n",
    "            game_reward += reward\n",
    "        \n",
    "            agent.remember(state[0], action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if frame_count % 10000 == 0:\n",
    "                print(\"Step count: {}\".format(frame_count))\n",
    "            \n",
    "            if done:\n",
    "                break            \n",
    "            \n",
    "            if frame_count > LEARNING_START_AFTER_STEPS:                \n",
    "                agent.train()\n",
    "                agent.soft_update_target_network()\n",
    "                \n",
    "            #if frame_count % UPDATE_TARGET_FREQUENCY == 0:\n",
    "            #    agent.hard_update_target_network()\n",
    "    \n",
    "        # Log episode reward\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar(\"epsilon\", agent.epsilon, step=i_episode)\n",
    "            tf.summary.scalar(\"game_reward\", game_reward, step=i_episode)        \n",
    "            \n",
    "        if i_episode % SAVE_EVERY_EPISODES == 0:\n",
    "            print(\"Save after {} episodes.\".format(i_episode))\n",
    "            agent.save()             \n",
    "        \n",
    "        game_rewards_deque.append(game_reward)\n",
    "        \n",
    "        if game_reward > max_reward:\n",
    "            print(\"Episode {}: Step {} reward {}: \".format(i_episode, frame_count, game_reward))\n",
    "            max_reward = game_reward\n",
    "            agent.save()        \n",
    "        \n",
    "        if np.mean(game_rewards_deque) >= SCORE_TO_SOLVE:\n",
    "            agent.save()\n",
    "            print(\"Solved in Episode {} Step {} reward {}: \".format(i_episode, frame_count, game_reward))\n",
    "            break      \n",
    "        \n",
    "    agent.save()\n",
    "    env.close()\n",
    "    \n",
    "train()\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished with score: 156.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(ENVIRONMENT)\n",
    "agent = build_Cartpole_v0_agent(pre_trained='model.h5')\n",
    "\n",
    "state = env.reset()\n",
    "final_reward = 0\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "while True:\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    action = agent.act(state)    \n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    final_reward += reward \n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    if done:\n",
    "        print(\"Episode finished with score: {}\".format(final_reward))\n",
    "        break\n",
    "env.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
