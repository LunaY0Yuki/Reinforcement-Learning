{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "from ReplayBuffer import ReplayBuffer\n",
    "from Model import TD3\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'BipedalWalker-v2'\n",
    "random_seed = 0\n",
    "log_interval = 10           # print avg reward after interval\n",
    "save_every = 500            # safe trained models after interval\n",
    "print_every = 1\n",
    "directory = \"./preTrained/\" # save trained models\n",
    "filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "\n",
    "# Hyperparameters\n",
    "max_episodes = 1000         # max num of episodes\n",
    "max_timesteps = 2000        # max timesteps in one episode\n",
    "\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 100            # num of transitions sampled from replay buffer\n",
    "exploration_noise = 0.1 \n",
    "polyak = 0.995              # target policy update parameter (1-tau)\n",
    "policy_noise = 0.2          # target policy smoothing noise\n",
    "noise_clip = 0.5\n",
    "policy_delay = 2            # delayed policy updates parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tobias\\appdata\\local\\conda\\conda\\envs\\openai\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "env.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Average Score: -1.16, Max: 0.15, Min: -100.00, Time: 2.48\n",
      "Episode 2, Average Score: -0.96, Max: 0.13, Min: -100.00, Time: 2.70\n",
      "Episode 3, Average Score: -0.11, Max: 0.03, Min: -0.42, Time: 38.83\n",
      "Episode 4, Average Score: -0.79, Max: -0.00, Min: -100.00, Time: 3.61\n",
      "Episode 5, Average Score: -0.83, Max: -0.00, Min: -100.00, Time: 3.38\n",
      "Episode 6, Average Score: -2.67, Max: -0.05, Min: -100.00, Time: 0.95\n",
      "Episode 7, Average Score: -0.87, Max: -0.01, Min: -100.00, Time: 3.15\n",
      "Episode 8, Average Score: -0.80, Max: 0.00, Min: -100.00, Time: 3.45\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    policy = TD3(state_dim, action_dim, max_action)\n",
    "    replay_buffer = ReplayBuffer()\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i_episode in range(1, max_episodes+1):\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_rewards = []\n",
    "        \n",
    "        for t in range(max_timesteps):\n",
    "            timestep = time.time()\n",
    "\n",
    "            # select action and add exploration noise:\n",
    "            action = policy.select_action(state)\n",
    "            action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
    "            action = action.clip(env.action_space.low, env.action_space.high)\n",
    "\n",
    "            # take action in env:\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            replay_buffer.add((state, action, reward, next_state, float(done)))            \n",
    "            episode_rewards.append(reward)            \n",
    "            state = next_state\n",
    "\n",
    "            # if i_episode is done then update policy:\n",
    "            if done or t==(max_timesteps-1):\n",
    "                policy.update(replay_buffer, t, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay)\n",
    "                break\n",
    "        \n",
    "        if i_episode % save_every == 0:\n",
    "            policy.save(directory, filename)\n",
    "        \n",
    "        avg_rewards = np.mean(episode_rewards)\n",
    "        if avg_rewards >= 300.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, score_average))\n",
    "            policy.save(directory, filename + '_solved')\n",
    "            break        \n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            min_rewards = np.min(episode_rewards)\n",
    "            max_rewards = np.max(episode_rewards)\n",
    "            \n",
    "            print('\\rEpisode {}, Average Score: {:.2f}, Max: {:.2f}, Min: {:.2f}, Time: {:.2f}'\\\n",
    "                  .format(i_episode, avg_rewards, max_rewards, min_rewards, time.time() - timestep), end=\"\\n\")\n",
    "            \n",
    "            episode_rewards = []\n",
    "        \n",
    "        scores.append(avg_rewards)\n",
    "            \n",
    "    return scores\n",
    "\n",
    "scores = train()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for ep in range(1, n_episodes+1):\n",
    "        ep_reward = 0\n",
    "        state = env.reset()\n",
    "        for t in range(max_timesteps):\n",
    "            action = policy.select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        print('Episode: {}\\tReward: {}'.format(ep, int(ep_reward)))\n",
    "        ep_reward = 0\n",
    "        env.close()        \n",
    "'''        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "env = gym.make(env_name)\n",
    "env.reset()\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "\n",
    "for _ in range(1000):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
