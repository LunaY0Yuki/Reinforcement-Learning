{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "from ReplayBuffer import ReplayBuffer\n",
    "from Model import TD3\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'BipedalWalker-v2'\n",
    "random_seed = 0\n",
    "save_every = 500            # safe trained models after interval\n",
    "print_every = 10\n",
    "directory = \"./preTrained/\" # save trained models\n",
    "filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "\n",
    "# Hyperparameters\n",
    "max_episodes = 20000         # max num of episodes\n",
    "max_timesteps = 1000000      # max timesteps in one episode\n",
    "\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 100            # num of transitions sampled from replay buffer\n",
    "exploration_noise = 0.1 \n",
    "polyak = 0.995              # target policy update parameter (1-tau)\n",
    "policy_noise = 0.2          # target policy smoothing noise\n",
    "noise_clip = 0.5\n",
    "policy_delay = 2            # delayed policy updates parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tobias\\appdata\\local\\conda\\conda\\envs\\openai\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "env.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Average Score: -116.61, Max: -92.33, Min: -173.37, Time: 2.14\n",
      "Episode 20, Average Score: -121.93, Max: -99.96, Min: -164.53, Time: 0.92\n",
      "Episode 30, Average Score: -119.44, Max: -110.86, Min: -124.92, Time: 1.07\n",
      "Episode 40, Average Score: -119.95, Max: -114.45, Min: -124.00, Time: 1.10\n",
      "Episode 50, Average Score: -113.57, Max: -102.66, Min: -134.67, Time: 1.66\n",
      "Episode 60, Average Score: -123.43, Max: -100.11, Min: -174.96, Time: 2.69\n",
      "Episode 70, Average Score: -105.36, Max: -98.47, Min: -110.86, Time: 1.40\n",
      "Episode 80, Average Score: -108.22, Max: -100.08, Min: -121.34, Time: 1.70\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    policy = TD3(state_dim, action_dim, max_action)\n",
    "    replay_buffer = ReplayBuffer()\n",
    "\n",
    "    scores = []\n",
    "    avg_reward = 0    \n",
    "    episode_rewards = []\n",
    "\n",
    "    for i_episode in range(1, max_episodes+1):\n",
    "        state = env.reset()\n",
    "        \n",
    "        ep_reward = 0\n",
    "        \n",
    "        for t in range(max_timesteps):\n",
    "            timestep = time.time()\n",
    "\n",
    "            # select action and add exploration noise:\n",
    "            action = policy.select_action(state)\n",
    "            action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
    "            action = action.clip(env.action_space.low, env.action_space.high)\n",
    "\n",
    "            # take action in env:\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            replay_buffer.add((state, action, reward, next_state, float(done)))\n",
    "            avg_reward += reward                       \n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # if i_episode is done then update policy:\n",
    "            if done or t==(max_timesteps-1):\n",
    "                policy.update(replay_buffer, t, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay)\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(ep_reward)\n",
    "        \n",
    "        avg_rewards = (avg_reward / print_every)        \n",
    "        if avg_rewards >= 300.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, avg_rewards))\n",
    "            policy.save(directory, filename + '_solved')\n",
    "            break        \n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            min_rewards = np.min(episode_rewards)\n",
    "            max_rewards = np.max(episode_rewards)            \n",
    "            print('\\rEpisode {}, Average Score: {:.2f}, Max: {:.2f}, Min: {:.2f}, Time: {:.2f}'\\\n",
    "                  .format(i_episode, avg_rewards, max_rewards, min_rewards, time.time() - timestep), end=\"\\n\")\n",
    "            \n",
    "            avg_reward = 0\n",
    "            episode_rewards = []\n",
    "            \n",
    "        if i_episode % save_every == 0:\n",
    "            policy.save(directory, filename)\n",
    "        \n",
    "        scores.append(ep_reward)\n",
    "            \n",
    "    return scores\n",
    "\n",
    "scores = train()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():        \n",
    "    n_episodes = 3\n",
    "    max_timesteps = 2000    \n",
    "    \n",
    "    filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "    filename += '_solved'    \n",
    "    directory = \"./preTrained/\"\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "    \n",
    "    policy = TD3(state_dim, action_dim, max_action)\n",
    "    \n",
    "    policy.load_actor(directory, filename)\n",
    "    \n",
    "    for ep in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        for t in range(max_timesteps):\n",
    "            action = policy.select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            env.render()            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        env.close()\n",
    "        \n",
    "test()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
