{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BipedalWalker-v2\n",
    "\n",
    "---\n",
    "In this notebook, you will implement a TD3 agent with OpenAI Gym's BipedalWalker-v2 environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import time\n",
    "from ReplayBuffer import ReplayBuffer\n",
    "from Model import TD3\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Instantiate the Environment\n",
    "Initialize the environment in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'BipedalWalker-v2'\n",
    "#env_name = 'BipedalWalkerHardcore-v2'\n",
    "#env_name = 'LunarLanderContinuous-v2'\n",
    "random_seed = 0\n",
    "save_every = 500            # safe trained models after interval\n",
    "print_every = 10\n",
    "#score_to_solve = 200.0\n",
    "score_to_solve = 300.0\n",
    "directory = \"./preTrained/\" # save trained models\n",
    "filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "continue_training = False\n",
    "\n",
    "max_episodes = 20000        # max num of episodes\n",
    "max_timesteps = 1500        # max timesteps in one episode\n",
    "\n",
    "exploration_noise = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "env.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Train the Agent with TD3\n",
    "Run the code cell below to train the agent from scratch. You are welcome to amend the supplied values of the parameters in the function, to try to see if you can get better performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Average Score: -126.86, Max: -107.33, Min: -162.65, Time: 1.51\n",
      "Episode 20, Average Score: -107.80, Max: -55.58, Min: -127.63, Time: 2.01\n",
      "Episode 30, Average Score: -113.73, Max: -102.27, Min: -123.65, Time: 2.77\n",
      "Episode 40, Average Score: -114.57, Max: -98.43, Min: -129.15, Time: 2.04\n",
      "Episode 50, Average Score: -117.91, Max: -100.52, Min: -130.59, Time: 1.51\n",
      "Episode 60, Average Score: -115.34, Max: -96.64, Min: -131.71, Time: 2.03\n",
      "Episode 70, Average Score: -114.30, Max: -99.07, Min: -128.08, Time: 1.41\n",
      "Episode 80, Average Score: -119.90, Max: -111.10, Min: -138.33, Time: 2.56\n",
      "Episode 90, Average Score: -119.16, Max: -103.27, Min: -130.48, Time: 3.28\n",
      "Episode 100, Average Score: -113.18, Max: -99.30, Min: -127.73, Time: 1.65\n",
      "Episode 110, Average Score: -122.22, Max: -109.77, Min: -133.88, Time: 1.70\n",
      "Episode 120, Average Score: -115.18, Max: -106.81, Min: -129.88, Time: 2.44\n",
      "Episode 130, Average Score: -111.52, Max: -101.57, Min: -127.39, Time: 2.11\n",
      "Episode 140, Average Score: -117.28, Max: -107.90, Min: -135.80, Time: 1.69\n",
      "Episode 150, Average Score: -119.35, Max: -101.42, Min: -137.84, Time: 2.71\n",
      "Episode 160, Average Score: -119.00, Max: -102.77, Min: -155.75, Time: 45.54\n",
      "Episode 170, Average Score: -114.78, Max: -102.02, Min: -141.04, Time: 45.68\n",
      "Episode 180, Average Score: -113.79, Max: -86.87, Min: -141.71, Time: 1.11\n",
      "Episode 190, Average Score: -111.91, Max: -110.97, Min: -113.39, Time: 1.16\n",
      "Episode 200, Average Score: -112.59, Max: -110.98, Min: -114.62, Time: 1.24\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    policy = TD3(state_dim, action_dim, max_action)\n",
    "    \n",
    "    if continue_training:\n",
    "        policy.load(directory, filename)\n",
    "    \n",
    "    replay_buffer = ReplayBuffer()\n",
    "\n",
    "    scores = []\n",
    "    avg_reward = 0    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    ep_rewards_deque = deque(maxlen=100)\n",
    "\n",
    "    for i_episode in range(1, max_episodes+1):\n",
    "        state = env.reset()\n",
    "        \n",
    "        ep_reward = 0\n",
    "        \n",
    "        timestep = time.time()\n",
    "        \n",
    "        for t in range(max_timesteps):\n",
    "            # select action and add exploration noise:\n",
    "            action = policy.select_action(state)\n",
    "            action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
    "            action = action.clip(env.action_space.low, env.action_space.high)\n",
    "\n",
    "            # take action in env:\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            replay_buffer.add((state, action, reward, next_state, float(done)))\n",
    "            avg_reward += reward                       \n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # if i_episode is done then update policy:\n",
    "            if done or t==(max_timesteps-1):\n",
    "                policy.update(replay_buffer, t)\n",
    "                break\n",
    "                \n",
    "        episode_rewards.append(ep_reward)\n",
    "        ep_rewards_deque.append(ep_reward)        \n",
    "        avg_rewards = (avg_reward / print_every)\n",
    "        \n",
    "        if np.mean(ep_rewards_deque) >= score_to_solve:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, avg_rewards))\n",
    "            policy.save(directory, filename + '_solved')\n",
    "            break        \n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            min_rewards = np.min(episode_rewards)\n",
    "            max_rewards = np.max(episode_rewards)            \n",
    "            print('\\rEpisode {}, Average Score: {:.2f}, Max: {:.2f}, Min: {:.2f}, Time: {:.2f}'\\\n",
    "                  .format(i_episode, avg_rewards, max_rewards, min_rewards, time.time() - timestep), end=\"\\n\")\n",
    "            \n",
    "            avg_reward = 0\n",
    "            episode_rewards = []\n",
    "            \n",
    "        if i_episode % save_every == 0:\n",
    "            policy.save(directory, filename)\n",
    "        \n",
    "        scores.append(ep_reward)\n",
    "            \n",
    "    return scores\n",
    "\n",
    "scores = train()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Watch a Smart Agent!\n",
    "In the next code cell, you will load the trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():        \n",
    "    n_episodes = 5    \n",
    "    \n",
    "    filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "    filename += '_solved'    \n",
    "    directory = \"./preTrained/\"\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "    \n",
    "    policy = TD3(state_dim, action_dim, max_action)\n",
    "    \n",
    "    policy.load_actor(directory, filename)\n",
    "    \n",
    "    for ep in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        for t in range(max_timesteps):\n",
    "            action = policy.select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            env.render()            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        env.close()\n",
    "        \n",
    "test()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
